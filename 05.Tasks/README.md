# Сбор и разметка данных

## Урок 5. Scrapy

### Домашнее задание

1. Найдите сайт, содержащий интересующий вас список или каталог. Это может быть список книг, фильмов, спортивных команд или что-то еще, что вас заинтересовало.
2. Создайте новый проект Scrapy и определите нового паука. С помощью атрибута start_urls укажите URL выбранной вами веб-страницы.
3. Определите метод парсинга для извлечения интересующих вас данных. Используйте селекторы XPath или CSS для навигации по HTML и извлечения данных. Возможно, потребуется извлечь данные с нескольких страниц или перейти по ссылкам на другие страницы.
4. Сохраните извлеченные данные в структурированном формате. Вы можете использовать оператор yield для возврата данных из паука, которые Scrapy может записать в файл в выбранном вами формате (например, JSON или CSV).
5. Конечным результатом работы должен быть код Scrapy Spider, а также пример выходных данных. Не забывайте соблюдать правила robots.txt и условия обслуживания веб-сайта, а также ответственно подходите к использованию веб-скрейпинга.

### Решение

1. Сайт выбрал, из предложенных и рассмотренных преподавателем на семинаре - hh.ru. И это не копирование кода с экрана монитора, т.к. исходники не выложены.

Данная работа с hh.ru охватывает: 

- вопросы подключения сторонних модулей к фреймворку Scrapy;
- использование сторонних служб в пайплайне с мидлваре;
- расширенный парсинг страниц, включающий полное описание вакансий;
- реализация, практически готового решения, для кадровых агенств, либо для собственных коммерческих и не очень целей.

2. Создание любого проекта Scrapy и его запуск из командной строки не зависит от среды разработки и виртуального окружения. Но, настройка, отладка и запуск в системной (общей) среде пайтон-окружения с Visual Studio Code отличается, от работы в виртуальном окружении PyCharm. Виртуальное окружение пайтона, отличный инструмент для изоляции приложения от пакетов и зависимостей соседних программ. Но, есть и свои недостатки: размер занимаемого дискового пространства, дополнительно время на установку и повышенный трафик. Виртуальное окружение пайтона, эквивалентно статической компиляции в С/C++, когда все необходимые зависимости и библиотеки компилируются вмести, получается независимый, но очень большой файл (пакет). Но, есть в СИ и динамическая компиляция приложения, позволяющая добиться его наименьшего размера. Динамическая сборка пакетов (*.deb, *.rpm, etc.) используется большинством дистрибутивов Linux.

Одной из проблем в результате использвания общего (системного) окружение пайтона, была фиксация корневой директории scrapy. При запуске из командной строки инструкции типа ```scrapy crawl hhru``` проблем не возникает, а вот в процессе отладки, пришлось повозиться с решением, а именно в файле [runner.py](https://github.com/allseenn/api/blob/main/05.Tasks/runner.py) прописать следующий код:

```
import os
os.chdir(os.path.dirname(os.path.abspath(__file__)))
```

В результате корневой директорией становится, та из которой запущен сам файл, в наше случае [runner.py](https://github.com/allseenn/api/blob/main/05.Tasks/runner.py).

3. Модуль [hhru.py](https://github.com/allseenn/api/blob/main/05.Tasks/jobparser/spiders/hhru.py) описывает класс основного паука. Тут реализовал, вывод в консоль запросов пользователю:

- Название искомой вакансии
- Ввод индекса региона для поиска

Запрошенные от пользователя данные используются в параметрах передаваемых в url. В результате чего можно не меняя код приложения осуществлять парсинг по разным профессиям и регионам.

В [hhru.py](https://github.com/allseenn/api/blob/main/05.Tasks/jobparser/spiders/hhru.py) реализовал основную логику парсинга, в результате чего собирается по 12 элементов (items):

- id - Во избежании дублирования записей в облачной базе Atlas Mondodb, "выдираю" уникальный id из ссылки на вакансию;
- vacancy_title - Наименование вакансии;
- vacancy_salary - оклад, парсится в несколько этапов: очистка от "висячих" пробелов, переименовария символя Р, в рубл., очистка от спец символов, с сохранением сум в целочисленном значении. На выходе получается список, который может содержать не только размер оклада, но и условия по его получению. Если никакой информации об окладе нет, с помощью исключений создается пустой список. 
- vacancy_experience - получаем список, который содержит помимо количества стажа в годах, еще условия работы (удаленная, полная занятость и т.д.)
- company_name - Название компании, в html коде часто дублируется в одном и том же теге, типа ООО "РогаИКопыта", ООО "РогаИКопыта", более того название компание может быть указано в как в сокращенном, так и в полном наименовани, на русском языке и одновременно на иностранном. Для сокращения информации и упорядочения применяю метод преобразования списка в множество и обратно, в результате остаются только уникальные значения.
- company_link - строка со ссылкой на профиль компании 
- company_rate - вещественное число, с рейтингом компании, на основе отзывов
- company_address - список адресов, компании. Она может иметь несколько адресов и филиалов, часто происходит дублирование типа: Москва, Москва. Для сокращения объема и уникальности применяю метод со множествами.
- description - самый емкий элемент, в начале хотел объединить все в одну большую строку, но при просмотре конечного файла json, либо при запросе в MongoDB, описание смотрится не читабельно, поэтому решил использовать список строк, что позволило добиться читаемости даже в выводе из базы либо в json-файле.
- vacancy_date=vacancy_date, 
- vacancy_link=vacancy_link, 
- vacancy_skills=vacancy_skills
