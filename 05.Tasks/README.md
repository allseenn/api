# Сбор и разметка данных

## Урок 5. Scrapy

### Домашнее задание

1. Найдите сайт, содержащий интересующий вас список или каталог. Это может быть список книг, фильмов, спортивных команд или что-то еще, что вас заинтересовало.
2. Создайте новый проект Scrapy и определите нового паука. С помощью атрибута start_urls укажите URL выбранной вами веб-страницы.
3. Определите метод парсинга для извлечения интересующих вас данных. Используйте селекторы XPath или CSS для навигации по HTML и извлечения данных. Возможно, потребуется извлечь данные с нескольких страниц или перейти по ссылкам на другие страницы.
4. Сохраните извлеченные данные в структурированном формате. Вы можете использовать оператор yield для возврата данных из паука, которые Scrapy может записать в файл в выбранном вами формате (например, JSON или CSV).
5. Конечным результатом работы должен быть код Scrapy Spider, а также пример выходных данных. Не забывайте соблюдать правила robots.txt и условия обслуживания веб-сайта, а также ответственно подходите к использованию веб-скрейпинга.

### Решение

1. Сайт выбрал, из предложенных и рассмотренных преподавателем на семинаре - hh.ru.
И это не копирование кода с экрана монитора, т.к. исходники не выложены. 

Данная работа с hh.ru охватывает: 
- вопросы подключения сторонних модулей к фреймворку Scrapy 
- использование сторонних служб в пайплайне с мидлваре
- расширенный парсинг страниц, включающий полное описание вакансий
- реализация, практически готового решения, для отдела кадров, либо для собственных коммерческих целей )

2. Создание проекта и его запуск из командной строки не зависит от проекта scrapy, среды разработки и виртуального окружения. Но, настройка, отладка и запуск в системной среде с Visual Studio Code отличны, от виртуального окружения PyCharm. Виртуальное окружение пайтона, отличный инструмент для изоляции приложения от пакетов и зависимостей соседних программ. Но, есть и свои недостатки: размер занимаемого дискового пространства, дополнительно время на установку и повышенный трафик. Виртуальное окружение пайтона, эквивалентно статической компиляции в С/C++, когда все необходимые зависимости и библиотеки компилируются вмести, получается независимый, но очень большой файл (пакет). Но, есть в СИ и динамическая компиляция приложения, позволяющая добиться его наименьшего размера. Динамическая сборка пакетов (*.deb, *.rpm, etc.) используется большинством дистрибутивов Linux. 
Одной из проблем в результате использвания общего (системного) окружение пайтона, была фиксация кореновй директории scrapy. При запуски из командной строки команды типа ```scrapy crawl hhru``` проблем не возникает, а вот в процессе отладки, пришлось повозиться с решением, а именно в файле runner.py прописать инструкции:
```
import os
os.chdir(os.path.dirname(os.path.abspath(__file__)))
```
В результате корневой директорией становится, та из которой запущен сам файл, в наше случае runner.py
3. 
