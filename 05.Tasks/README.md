# Сбор и разметка данных

## Урок 5. Scrapy

### Домашнее задание

1. Найдите сайт, содержащий интересующий вас список или каталог. Это может быть список книг, фильмов, спортивных команд или что-то еще, что вас заинтересовало.
2. Создайте новый проект Scrapy и определите нового паука. С помощью атрибута start_urls укажите URL выбранной вами веб-страницы.
3. Определите метод парсинга для извлечения интересующих вас данных. Используйте селекторы XPath или CSS для навигации по HTML и извлечения данных. Возможно, потребуется извлечь данные с нескольких страниц или перейти по ссылкам на другие страницы.
4. Сохраните извлеченные данные в структурированном формате. Вы можете использовать оператор yield для возврата данных из паука, которые Scrapy может записать в файл в выбранном вами формате (например, JSON или CSV).
5. Конечным результатом работы должен быть код Scrapy Spider, а также пример выходных данных. Не забывайте соблюдать правила robots.txt и условия обслуживания веб-сайта, а также ответственно подходите к использованию веб-скрейпинга.

## Решение

https://github.com/allseenn/api/tree/main/05.Tasks

### 1. Выбор сайта

Сайт выбрал, из предложенных и рассмотренных преподавателем на семинаре - hh.ru. И это не копирование кода с экрана монитора, т.к. исходники не выложены.

Данная работа с hh.ru охватывает: 

- вопросы подключения сторонних модулей к фреймворку Scrapy;
- использование сторонних служб в пайплайне с мидлваре;
- расширенный парсинг страниц, включающий полное описание вакансий;
- реализация, практически готового решения, для кадровых агенств, либо для собственных коммерческих и не очень целей.

### 2. Проект Scrapy

Создание любого проекта Scrapy и его запуск из командной строки не зависит от среды разработки и виртуального окружения. Но, настройка, отладка и запуск в системной (общей) среде пайтон-окружения с Visual Studio Code отличается, от работы в виртуальном окружении PyCharm. Виртуальное окружение пайтона, отличный инструмент для изоляции приложения от пакетов и зависимостей соседних программ. Но, есть и свои недостатки: размер занимаемого дискового пространства, дополнительно время на установку и повышенный трафик. Виртуальное окружение пайтона, эквивалентно статической компиляции в С/C++, когда все необходимые зависимости и библиотеки компилируются вмести, получается независимый, но очень большой файл (пакет). Но, есть в СИ и динамическая компиляция приложения, позволяющая добиться его наименьшего размера. Динамическая сборка пакетов (*.deb, *.rpm, etc.) используется большинством дистрибутивов Linux.

Одной из проблем в результате использвания общего (системного) окружение пайтона, была фиксация корневой директории scrapy. При запуске из командной строки инструкции типа ```scrapy crawl hhru``` проблем не возникает, а вот в процессе отладки, пришлось повозиться с решением, а именно в файле [runner.py](https://github.com/allseenn/api/blob/main/05.Tasks/runner.py) прописать следующий код:

```
import os
os.chdir(os.path.dirname(os.path.abspath(__file__)))
```

В результате корневой директорией становится, та из которой запущен сам файл, в наше случае [runner.py](https://github.com/allseenn/api/blob/main/05.Tasks/runner.py).

### 3. Парсинг 

Модуль [hhru.py](https://github.com/allseenn/api/blob/main/05.Tasks/jobparser/spiders/hhru.py) описывает класс основного паука. Тут реализовал, вывод в консоль запросов пользователю:

- Название искомой вакансии
- Ввод индекса региона для поиска

Запрошенные от пользователя данные используются в параметрах передаваемых в url. В результате чего можно не меняя код приложения осуществлять парсинг по разным профессиям и регионам.

В [hhru.py](https://github.com/allseenn/api/blob/main/05.Tasks/jobparser/spiders/hhru.py) реализовал основную логику парсинга, в результате чего собирается по 12 элементов (items):

- id - Во избежании дублирования записей в облачной базе Atlas Mondodb, "выдираю" уникальный id из ссылки на вакансию;
- vacancy_title - Наименование вакансии;
- vacancy_salary - оклад, парсится в несколько этапов: очистка от "висячих" пробелов, переименовария символя Р, в рубл., очистка от спец символов, с сохранением сум в целочисленном значении. На выходе получается список, который может содержать не только размер оклада, но и условия по его получению. Если никакой информации об окладе нет, с помощью исключений создается пустой список. 
- vacancy_experience - получаем список, который содержит помимо количества стажа в годах, еще условия работы (удаленная, полная занятость и т.д.)
- company_name - Название компании, в html коде часто дублируется в одном и том же теге, типа ООО "РогаИКопыта", ООО "РогаИКопыта", более того название компание может быть указано в как в сокращенном, так и в полном наименовани, на русском языке и одновременно на иностранном. Для сокращения информации и упорядочения применяю метод преобразования списка в множество и обратно, в результате остаются только уникальные значения.
- company_link - строка со ссылкой на профиль компании 
- company_rate - вещественное число, с рейтингом компании, на основе отзывов
- company_address - список адресов, компании. Она может иметь несколько адресов и филиалов, часто происходит дублирование типа: Москва, Москва. Для сокращения объема и уникальности применяю метод со множествами.
- description - самый емкий элемент, в начале хотел объединить все в одну большую строку, но при просмотре конечного файла json, либо при запросе в MongoDB, описание смотрится не читабельно, поэтому решил использовать список строк, что позволило добиться читаемости даже в выводе из базы либо в [json-файле](https://github.com/allseenn/api/blob/main/05.Tasks/pics/01.png).
- vacancy_date - дата парсится, чистится от мусора и преобразуется в формат даты
- vacancy_link - строка, содержащую ссылку в интернете на данную вакансию
- vacancy_skills - список тэгов с требуемыми навыками

### 4. Данные

В процессе парсинга, настроил scrapy на отправку обработанных данных, на облачную службу Atlas MongoDB. Но, для выгрузки нужной коллекции из базы данных в [json-файл](https://raw.githubusercontent.com/allseenn/api/main/05.Tasks/hhru.json) написал [небольшой скрипт](https://github.com/allseenn/api/blob/main/05.Tasks/download.py).

В результаты скапинга, было обработано и сохранено в базу около 800 вакансий, для этого фреймворком было совершено около 1000 запросов к сайту hh.ru. Как правило, такие крупные проекты испытывают колосальную нагрузку на свои вычислительные мощности, от большого наплыва посетителей в поисках или размещениях вакансий по все стране. И не мало хлопот им доставляют скрейперы. Особенно после семинаров, в которых показано как обработывать их сайт. 
Естественно у hh.ru стоит защита от таких действий, в результате первого сканирования, мой ip-адрес был заблокирован. Для успешной миссии по скрейпингу тысячи вакансий, хорошо подходить прокси карусели.

### 5. Результат

Прокси карусель ораганизована с помощью модуля под *scrapy_rotating_proxies*, для получения сотни прокси серверов, воспользовался одним из многочисленных сайтов https://proxyscrape.com/, доступ к которому возможен только по ВПН, либо через другой прокси, т.к. для нашей страны есть ограничения. Тем не менне, сайт дайт 30-дневный бесплатный доступ к сотням прокси адресов из америки и канады. Не смотря на блокировку страны, доступ к их api свободный.
Есть множество бесплатных сайтов и репозиториев на гитхабе, где выложены в открытый доступ списки прокси-серверов. Но, как паказывает практика, такие серверы часто перегружены, либо не работают. Сайты, с бесплатными триалами, как правило, в рекламных целях дают гарантированно рабочие прокси. Кстати написал [скрипт](https://github.com/allseenn/api/blob/main/05.Lecture/proxy.py), загружающий список и проверяющий прокси на доступность.
Но, используя модуль *scrapy_rotating_proxies*, необходимости в таком скрипте нет. Он может, проверять доступность серверов и пускать трафик через доступные. Он, самостоятельно отслеживает, не только доступность, но и контролирует карусель, и следит за возвращаемым кодом от скрейпируемого сайта, и в случае не положительных кодов, контролирует скорость вращения на карусели.

Для установки модуля испольуем команду:
```pip install scrapy_rotating_proxies```

Далее, в [файле настроек](https://github.com/allseenn/api/blob/main/05.Tasks/jobparser/pipelines.py), необходимо добавить слеудющие строки:

```
DOWNLOADER_MIDDLEWARES = {
   'rotating_proxies.middlewares.RotatingProxyMiddleware': 100,
   'rotating_proxies.middlewares.BanDetectionMiddleware': 100,
}
```

Там же необходимо прописать путь до файла со списком прокси:

```
ROTATING_PROXY_LIST_PATH = '../../../proxy_list.txt'
```

Использовал всего 100 прокси, можно больше для масштабных проектов. Выставил относительно щадящие настроки:

```
USER_AGENT = ua().random
CONCURRENT_REQUESTS = 32
DOWNLOAD_DELAY = 0.5
```

В результате, через час работы паука на карусели прокси, база из 800 вакансий была собрана:

- [Фотообразец данных](https://github.com/allseenn/api/blob/main/05.Tasks/pics/01.png)
- [JSON-файл вакансий](https://github.com/allseenn/api/blob/main/05.Tasks/hhru.json)